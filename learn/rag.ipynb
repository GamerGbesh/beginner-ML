{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Philemon\\anaconda3\\envs\\transform\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Philemon\\anaconda3\\envs\\transform\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import ollama\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: doc1\n",
      "Add of existing embedding ID: doc2\n",
      "Add of existing embedding ID: doc3\n",
      "Add of existing embedding ID: doc1\n",
      "Insert of existing embedding ID: doc1\n",
      "Add of existing embedding ID: doc2\n",
      "Insert of existing embedding ID: doc2\n",
      "Add of existing embedding ID: doc3\n",
      "Insert of existing embedding ID: doc3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents added to ChromaDB!\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"documents\")\n",
    "\n",
    "# Load an embedding model\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Sample documents to add\n",
    "documents = [\n",
    "    {\"id\": \"doc1\", \"text\": \"Quantum computing is the future of cryptography.\"},\n",
    "    {\"id\": \"doc2\", \"text\": \"Neural networks are the foundation of deep learning.\"},\n",
    "    {\"id\": \"doc3\", \"text\": \"The Large Hadron Collider is the world's most powerful particle accelerator.\"}\n",
    "]\n",
    "\n",
    "# Convert text into embeddings and store in Chroma\n",
    "for doc in documents:\n",
    "    embedding = embed_model.encode(doc[\"text\"]).tolist()\n",
    "    collection.add(ids=[doc[\"id\"]], embeddings=[embedding], metadatas=[{\"text\": doc[\"text\"]}])\n",
    "\n",
    "print(\"Documents added to ChromaDB!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant doc: Neural networks are the foundation of deep learning.\n",
      "Relevant doc: The Large Hadron Collider is the world's most powerful particle accelerator.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is deep learning?\"\n",
    "query_embedding = embed_model.encode(query).tolist()\n",
    "\n",
    "results = collection.query(query_embedding, n_results=2)\n",
    "\n",
    "\n",
    "for doc in results[\"metadatas\"][0]:\n",
    "    print(f\"Relevant doc: {doc['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama's Response: The provided information doesn't explain what deep learning is. It simply states that neural networks are the foundation of deep learning. \n",
      "\n",
      "To answer the question \"What is deep learning?\" we'd need more context. \n",
      "\n",
      "**Therefore, I cannot answer the question based on the given information.**\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate prompt with retrieved documents\n",
    "retrieved_docs = \"\\n\".join([doc[\"text\"] for doc in results[\"metadatas\"][0]])\n",
    "prompt = f\"Use the following information to answer the question:\\n{retrieved_docs}\\n\\nQuestion: {query}\"\n",
    "\n",
    "# Send to Ollama\n",
    "response = ollama.chat(model=\"gemma3:1b\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "print(\"Ollama's Response:\", response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: doc4\n",
      "Insert of existing embedding ID: doc4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added successfully\n"
     ]
    }
   ],
   "source": [
    "doc = fitz.open(\"Mensah Philemon's CV.pdf\")\n",
    "text = \"\"\n",
    "for page in doc:\n",
    "    text += page.get_text()\n",
    "document = {\"id\": \"doc4\", \"text\": text}\n",
    "embedding = embed_model.encode(document[\"text\"]).tolist()\n",
    "collection.add(ids=[document[\"id\"]], embeddings=[embedding], metadatas=[{\"text\": document[\"text\"]}])\n",
    "print(\"Added successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama's Response: The name of the student is Mensah Philemon Edem Yao.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the name of the student?\"\n",
    "query_embedding = embed_model.encode(query).tolist()\n",
    "\n",
    "result = collection.query(query_embedding, n_results=1)\n",
    "retrieved = result[\"metadatas\"][0][0][\"text\"]\n",
    "prompt = f\"Use the following information to answer the question:\\n{retrieved}\\n\\nQuestion: {query}\"\n",
    "response = ollama.chat(model=\"gemma3:1b\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(\"Ollama's Response:\", response[\"message\"][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
